{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "DELTA_T = 0.1  # Time step (seconds)\n",
    "AREA_LIMIT = 3.0  # 3x3 square\n",
    "NUM_PARTICLES = 500  # Number of particles\n",
    "MOTION_NOISE = [0.1, 0.1, np.deg2rad(1)]  # Noise in [x, y, theta]\n",
    "LANDMARKS = [[0.5, 0.5], [2.5, 0.5], [2.5, 2.5], [0.5, 2.5]]  # Landmarks in the area\n",
    "\n",
    "# Load tf_data (ground truth as input for now)\n",
    "def load_tf_data(filename):\n",
    "    \"\"\"Load tf_data (ground truth) from a CSV file.\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    return df['translation_x'], df['translation_y'], df['rotation_z']\n",
    "\n",
    "# Particle Filter Class\n",
    "class ParticleFilter:\n",
    "    def __init__(self, num_particles, area_limit):\n",
    "        self.num_particles = num_particles\n",
    "        self.area_limit = area_limit\n",
    "        # Initialize particles uniformly within the area\n",
    "        self.particles = np.zeros((num_particles, 3))  # [x, y, theta]\n",
    "        self.particles[:, 0] = np.random.uniform(0, area_limit, num_particles)  # x\n",
    "        self.particles[:, 1] = np.random.uniform(0, area_limit, num_particles)  # y\n",
    "        self.particles[:, 2] = np.random.uniform(-np.pi, np.pi, num_particles)  # theta\n",
    "        self.weights = np.ones(num_particles) / num_particles\n",
    "\n",
    "    def predict(self, control_input):\n",
    "        \"\"\"Propagate particles using the motion model with noise.\"\"\"\n",
    "        v, omega = control_input  # Linear and angular velocity\n",
    "        noise = np.random.normal(0, MOTION_NOISE, (self.num_particles, 3))\n",
    "        self.particles[:, 2] += omega * DELTA_T + noise[:, 2]  # Update theta\n",
    "        self.particles[:, 2] = (self.particles[:, 2] + np.pi) % (2 * np.pi) - np.pi  # Normalize angle\n",
    "        self.particles[:, 0] += (v * np.cos(self.particles[:, 2]) * DELTA_T + noise[:, 0])  # Update x\n",
    "        self.particles[:, 1] += (v * np.sin(self.particles[:, 2]) * DELTA_T + noise[:, 1])  # Update y\n",
    "        # Clamp particles to the area limit\n",
    "        self.particles[:, 0] = np.clip(self.particles[:, 0], 0, self.area_limit)\n",
    "        self.particles[:, 1] = np.clip(self.particles[:, 1], 0, self.area_limit)\n",
    "\n",
    "    def update(self, measurements, measurement_noise, landmarks):\n",
    "        \"\"\"Update particle weights based on measurements to landmarks.\"\"\"\n",
    "        for i, landmark in enumerate(landmarks):\n",
    "            # Calculate expected distances to this landmark for all particles\n",
    "            expected_distances = np.sqrt((self.particles[:, 0] - landmark[0])**2 + \n",
    "                                         (self.particles[:, 1] - landmark[1])**2)\n",
    "            # Actual distance measurement for this landmark\n",
    "            actual_distance = measurements[i]\n",
    "            \n",
    "            # Calculate weights based on Gaussian likelihood\n",
    "            self.weights *= np.exp(-0.5 * ((expected_distances - actual_distance) / measurement_noise[0])**2)\n",
    "        \n",
    "        # Normalize weights\n",
    "        self.weights /= np.sum(self.weights)\n",
    "\n",
    "    def resample(self):\n",
    "        \"\"\"Resample particles based on their weights.\"\"\"\n",
    "        indices = np.random.choice(range(self.num_particles), size=self.num_particles, p=self.weights)\n",
    "        self.particles = self.particles[indices]\n",
    "        self.weights.fill(1.0 / self.num_particles)  # Reset weights\n",
    "\n",
    "    def estimate(self):\n",
    "        \"\"\"Estimate the state as the weighted mean of the particles.\"\"\"\n",
    "        mean = np.average(self.particles, weights=self.weights, axis=0)\n",
    "        covariance = np.cov(self.particles.T, aweights=self.weights)\n",
    "        return mean, covariance\n",
    "\n",
    "# Visualization Function\n",
    "def plot_particles(particles, weights, ground_truth):\n",
    "    \"\"\"Plot particles, their weights, and the ground truth position.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(particles[:, 0], particles[:, 1], s=weights * 1000, alpha=0.4, label='Particles')\n",
    "    plt.scatter(ground_truth[0], ground_truth[1], color='red', label='Ground Truth', s=100)\n",
    "    for landmark in LANDMARKS:\n",
    "        plt.scatter(landmark[0], landmark[1], color='green', label='Landmark', s=100)\n",
    "    plt.xlim(-0.5, AREA_LIMIT + 0.5)\n",
    "    plt.ylim(-0.5, AREA_LIMIT + 0.5)\n",
    "    plt.xlabel('X Position (m)')\n",
    "    plt.ylabel('Y Position (m)')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Simulate Landmark Measurements\n",
    "def simulate_landmark_measurements(ground_truth, landmarks, noise_std):\n",
    "    \"\"\"Simulate measurements from the ground truth position to landmarks.\"\"\"\n",
    "    measurements = []\n",
    "    for landmark in landmarks:\n",
    "        distance = np.sqrt((ground_truth[0] - landmark[0])**2 + (ground_truth[1] - landmark[1])**2)\n",
    "        noisy_distance = distance + np.random.normal(0, noise_std)\n",
    "        measurements.append(noisy_distance)\n",
    "    return measurements\n",
    "\n",
    "# Main Simulation Example\n",
    "# Load tf_data (assuming the file is named 'tf_data.csv')\n",
    "tf_x, tf_y, tf_theta = load_tf_data('tf_data.csv')\n",
    "\n",
    "# Initialize Particle Filter\n",
    "pf = ParticleFilter(NUM_PARTICLES, AREA_LIMIT)\n",
    "\n",
    "# Simulate Particle Filter with tf_data\n",
    "ground_truth = np.vstack((tf_x, tf_y, tf_theta)).T\n",
    "for t in range(len(ground_truth) - 1):\n",
    "    # Use ground truth motion as control input (for now)\n",
    "    control_input = [0.5, np.deg2rad(5)]  # Example control input (v, omega)\n",
    "\n",
    "    # Prediction Step\n",
    "    pf.predict(control_input)\n",
    "\n",
    "    # Simulate landmark measurements from the ground truth\n",
    "    measurement = simulate_landmark_measurements(ground_truth[t], LANDMARKS, noise_std=0.2)\n",
    "\n",
    "    # Update Step\n",
    "    pf.update(measurement, measurement_noise=[0.2], landmarks=LANDMARKS)\n",
    "\n",
    "    # Resampling Step\n",
    "    pf.resample()\n",
    "\n",
    "    # Visualization\n",
    "    plot_particles(pf.particles, pf.weights, ground_truth[t, :2])\n",
    "\n",
    "# Final Estimate\n",
    "final_mean, final_covariance = pf.estimate()\n",
    "print(\"Final Estimated State:\", final_mean)\n",
    "print(\"Final Estimated Covariance:\", final_covariance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
